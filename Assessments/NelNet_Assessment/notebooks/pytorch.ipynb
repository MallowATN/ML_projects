{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/wombi_employees.csv')\n",
    "df['most_recent_income']= df['most_recent_income'].replace(',','', regex=True).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = df[['most_recent_income','problem_solving_skill']]\n",
    "# df_new1 = df[['technology_skill','total_jobs','english_skill']]\n",
    "\n",
    "# imputer = IterativeImputer()\n",
    "# imputed_df = pd.DataFrame(imputer.fit_transform(df_new))\n",
    "# imputed_df1 = pd.DataFrame(imputer.fit_transform(df_new1))\n",
    "\n",
    "# imputed_df = imputed_df.rename({0:'most_recent_income',1:'problem_solving_skill'}, axis=1)\n",
    "# imputed_df1 = imputed_df1.rename({0:'technology_skill',1:'total_jobs',2:'english_skill'}, axis=1)\n",
    "\n",
    "# df['most_recent_income'] = imputed_df['most_recent_income']\n",
    "# df['problem_solving_skill'] = imputed_df['problem_solving_skill']\n",
    "# df['technology_skill'] = imputed_df1['technology_skill']\n",
    "# df['total_jobs'] = imputed_df1['total_jobs']\n",
    "# df['english_skill'] = imputed_df1['english_skill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['problem_solving_skill','technology_skill','english_skill','age','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('score',axis=1)\n",
    "y = df['score']\n",
    "\n",
    "# print(X.select_dtypes(include=['number']).columns)\n",
    "X_num = X[['wombus_id', 'age', 'college_degree', 'problem_solving_skill',\n",
    "       'technology_skill', 'english_skill', 'most_recent_income', 'total_jobs']]\n",
    "# print(X.select_dtypes(include=['object']).columns)\n",
    "X_ohe_cat = X[['birth_continent', 'gender', 'shirt_color_preference',\n",
    "              'remote_work_preference','industry_preference']]\n",
    "X_ord_cat = X[['customer_exp_preference', 'work_env_preference',\n",
    "              'personal_growth_preference', 'honest_communication_preference',\n",
    "              'community_service_preference']]\n",
    "\n",
    "# Create the numerical pipeline\n",
    "num_pipeline = make_column_transformer(\n",
    "       (SimpleImputer(strategy=\"most_frequent\"),['age']),\n",
    "       (SimpleImputer(strategy=\"mean\"),['problem_solving_skill', 'technology_skill','english_skill']),\n",
    "       (SimpleImputer(strategy=\"most_frequent\"),['total_jobs']),\n",
    "       (SimpleImputer(strategy=\"mean\"),['most_recent_income']),\n",
    "       remainder='drop')\n",
    "\n",
    "# Create the categorical pipeline for both one-hot encoding and ordinal encoding\n",
    "cat_ohe_pipeline = Pipeline([\n",
    "                    ('imputer',SimpleImputer(strategy='most_frequent'))\n",
    "                    ,('ohe',OneHotEncoder())])\n",
    "cat_ord_pipeline = Pipeline([\n",
    "                    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal',OrdinalEncoder())])\n",
    "# Add standardscaler to the numeric pipeline\n",
    "num_pipeline1 = Pipeline([('imp_num_pipeline',num_pipeline),\n",
    "                        ('scaler',StandardScaler())])\n",
    "\n",
    "# Use ColumnTransformer to combine pipelines and create a workflow for the ML process\n",
    "\n",
    "num_attribs = list(X_num)\n",
    "cat_ohe_attribs = list(X_ohe_cat)\n",
    "cat_ord_attribs = list(X_ord_cat)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "            ('num', num_pipeline1, num_attribs),\n",
    "            ('cat_ohe', cat_ohe_pipeline,cat_ohe_attribs),\n",
    "            ('cat_ord',cat_ord_pipeline,cat_ord_attribs)])\n",
    "\n",
    "# fit transform your X\n",
    "# X_prepared = num_pipeline1.fit_transform(X)\n",
    "X_prepared = full_pipeline.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What our model does:\n",
    "* It starts with random values (weights and bias)\n",
    "* Looks at the training data and adjust the random values to better represent (or get closer to) the ideal value of the weights and bias\n",
    "\n",
    "<br>and how it does so is by two main algorithm:\n",
    "* Gradiant Descent (reason why requires_grad = True)... keeps track of the weights and bias parameter and update them in combination of gradient descent and backpropagation.\n",
    "* Backpropagation\n",
    "\n",
    "<br>\n",
    "The whole idea for training is for a model to move from unknown parameter to some known parameters (from a poor representation of data to a better representation of data <br>\n",
    "One way to measure how poor or how wrong our model predictions are is to use a loss function or cost function. (a function to measure how wrong our model predictions are to true value)<br>\n",
    "Optimizers = takes into account the loss of a model and adjusts the model's parameters (weights and bias)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "0. Loop through data\n",
    "1. Forward pass (this involves data moving through our model's forward())\n",
    "2. Calculate the loss (compare forward pass predictions to groudn truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - moves backward through the network to calculate the gradients of each parameters with respect to the loss (backpropagation)\n",
    "5. Optimizer Step - use the optimzier to adjust our model's parameters to try and improve the loss (gradient descent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Model Building Essentials\n",
    "* torch.nn - contains all the building for computational graphs (a nearal network can be considered a computational graph)\n",
    "* torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us\n",
    "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
    "* torch.optim - this is where the optimizer in PyTorch live, they will help with the gradient descent\n",
    "* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_prepared.astype(np.float32))\n",
    "y = torch.from_numpy(y.values.astype(np.float32)).view(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=X.shape[1]\n",
    "output_dim=1\n",
    "param_grid = {'hidden_dim':[2,4,6,8,10,12,14,16,18,20], 'dropout_rate':[0.1,0.2,0.3]}\n",
    "hidden_dim_range = [2,4,6,8,10,12,14,16,18,20]\n",
    "dropout_rate_range = [0.1, 0.2, 0.3]\n",
    "best_score = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8*len(X))\n",
    "X_train, y_train = X[:train_split],y[:train_split]\n",
    "X_test, y_test = X[train_split:],y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, dropout_rate):\n",
    "        super(MultivariateLinearRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_layers)\n",
    "        self.linear2 = nn.Linear(hidden_layers,hidden_layers)\n",
    "        self.linear3 = nn.Linear(hidden_layers,output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# Set up loss fucntion\n",
    "loss_fn = nn.L1Loss() # minimize the distance between predictions and true value... keeps increase weights until weights reduce loss.. decrease the bias and finds loss increase.. increase bias\n",
    "#set up optimizer(stocastic gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | Loss: 1074.91015625 | Test loss: 735.7568969726562\n",
      "Epoch: 2000 | Loss: 949.537353515625 | Test loss: 684.03759765625\n",
      "Epoch: 3000 | Loss: 853.565673828125 | Test loss: 664.6493530273438\n",
      "Epoch: 4000 | Loss: 768.2911376953125 | Test loss: 648.0682983398438\n",
      "Epoch: 5000 | Loss: 704.399169921875 | Test loss: 635.626708984375\n",
      "Epoch: 6000 | Loss: 662.5659790039062 | Test loss: 627.2640991210938\n",
      "Epoch: 7000 | Loss: 637.9906005859375 | Test loss: 622.3877563476562\n",
      "Epoch: 8000 | Loss: 627.1609497070312 | Test loss: 620.39501953125\n",
      "Epoch: 9000 | Loss: 624.265625 | Test loss: 620.2694702148438\n",
      "Epoch: 10000 | Loss: 624.2367553710938 | Test loss: 620.6326904296875\n",
      "Epoch: 11000 | Loss: 624.3193359375 | Test loss: 620.5252685546875\n",
      "Epoch: 12000 | Loss: 624.1719360351562 | Test loss: 620.5164794921875\n",
      "Epoch: 13000 | Loss: 624.2649536132812 | Test loss: 620.5256958007812\n",
      "Epoch: 14000 | Loss: 624.2413330078125 | Test loss: 620.5756225585938\n",
      "Epoch: 15000 | Loss: 624.16064453125 | Test loss: 620.5670166015625\n",
      "Epoch: 16000 | Loss: 624.2991333007812 | Test loss: 620.6311645507812\n",
      "Epoch: 17000 | Loss: 624.360107421875 | Test loss: 620.590576171875\n",
      "Epoch: 18000 | Loss: 624.2098388671875 | Test loss: 620.5732421875\n",
      "Epoch: 19000 | Loss: 624.2489013671875 | Test loss: 620.6012573242188\n",
      "Epoch: 20000 | Loss: 624.2896118164062 | Test loss: 620.6016235351562\n",
      "Epoch: 0 | Loss: 3432.17919921875 | Test loss: 3427.5703125\n",
      "Epoch: 1000 | Loss: 1422.85107421875 | Test loss: 1124.36572265625\n",
      "Epoch: 2000 | Loss: 1317.1192626953125 | Test loss: 1081.154296875\n",
      "Epoch: 3000 | Loss: 1232.03759765625 | Test loss: 1064.3739013671875\n",
      "Epoch: 4000 | Loss: 1158.521484375 | Test loss: 1050.0299072265625\n",
      "Epoch: 5000 | Loss: 1103.1802978515625 | Test loss: 1039.006591796875\n",
      "Epoch: 6000 | Loss: 1064.119140625 | Test loss: 1031.434326171875\n",
      "Epoch: 7000 | Loss: 1041.96142578125 | Test loss: 1026.934326171875\n",
      "Epoch: 8000 | Loss: 1031.6834716796875 | Test loss: 1024.9188232421875\n",
      "Epoch: 9000 | Loss: 1028.901611328125 | Test loss: 1024.789794921875\n",
      "Epoch: 10000 | Loss: 1028.7215576171875 | Test loss: 1025.0130615234375\n",
      "Epoch: 11000 | Loss: 1028.7252197265625 | Test loss: 1024.9998779296875\n",
      "Epoch: 12000 | Loss: 1028.9764404296875 | Test loss: 1025.142578125\n",
      "Epoch: 13000 | Loss: 1028.84423828125 | Test loss: 1025.0009765625\n",
      "Epoch: 14000 | Loss: 1028.687255859375 | Test loss: 1025.067138671875\n",
      "Epoch: 15000 | Loss: 1028.658935546875 | Test loss: 1025.077880859375\n",
      "Epoch: 16000 | Loss: 1028.9010009765625 | Test loss: 1024.9830322265625\n",
      "Epoch: 17000 | Loss: 1028.8828125 | Test loss: 1025.026611328125\n",
      "Epoch: 18000 | Loss: 1028.8138427734375 | Test loss: 1025.0784912109375\n",
      "Epoch: 19000 | Loss: 1028.6834716796875 | Test loss: 1024.9539794921875\n",
      "Epoch: 20000 | Loss: 1028.898193359375 | Test loss: 1025.0338134765625\n",
      "Epoch: 0 | Loss: 3422.404541015625 | Test loss: 3417.6494140625\n",
      "Epoch: 1000 | Loss: 2978.69384765625 | Test loss: 2974.40478515625\n",
      "Epoch: 2000 | Loss: 2639.4443359375 | Test loss: 2635.572998046875\n",
      "Epoch: 3000 | Loss: 2385.689453125 | Test loss: 2382.198974609375\n",
      "Epoch: 4000 | Loss: 2204.174560546875 | Test loss: 2201.03173828125\n",
      "Epoch: 5000 | Loss: 2084.222412109375 | Test loss: 2081.392333984375\n",
      "Epoch: 6000 | Loss: 2015.0467529296875 | Test loss: 2012.4852294921875\n",
      "Epoch: 7000 | Loss: 1983.65283203125 | Test loss: 1981.2989501953125\n",
      "Epoch: 8000 | Loss: 1974.439208984375 | Test loss: 1972.2159423828125\n",
      "Epoch: 9000 | Loss: 1973.2125244140625 | Test loss: 1971.0430908203125\n",
      "Epoch: 10000 | Loss: 1973.1707763671875 | Test loss: 1971.0120849609375\n",
      "Epoch: 11000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 12000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 13000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 14000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 15000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 16000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 17000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 18000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 19000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 20000 | Loss: 1973.170654296875 | Test loss: 1971.0126953125\n",
      "Epoch: 0 | Loss: 3418.839599609375 | Test loss: 3413.6923828125\n",
      "Epoch: 1000 | Loss: 2626.110107421875 | Test loss: 2621.790283203125\n",
      "Epoch: 2000 | Loss: 2019.1971435546875 | Test loss: 2015.6209716796875\n",
      "Epoch: 3000 | Loss: 1564.4632568359375 | Test loss: 1561.56494140625\n",
      "Epoch: 4000 | Loss: 1238.42626953125 | Test loss: 1236.1478271484375\n",
      "Epoch: 5000 | Loss: 1022.2191162109375 | Test loss: 1020.4982299804688\n",
      "Epoch: 6000 | Loss: 896.8295288085938 | Test loss: 895.5883178710938\n",
      "Epoch: 7000 | Loss: 839.361328125 | Test loss: 838.4938354492188\n",
      "Epoch: 8000 | Loss: 822.191162109375 | Test loss: 821.5596923828125\n",
      "Epoch: 9000 | Loss: 819.8313598632812 | Test loss: 819.2991333007812\n",
      "Epoch: 10000 | Loss: 819.7467041015625 | Test loss: 819.2346801757812\n",
      "Epoch: 11000 | Loss: 819.7462768554688 | Test loss: 819.235595703125\n",
      "Epoch: 12000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 13000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 14000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 15000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 16000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 17000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 18000 | Loss: 819.7462768554688 | Test loss: 819.2356567382812\n",
      "Epoch: 19000 | Loss: 819.7462768554688 | Test loss: 819.235595703125\n",
      "Epoch: 20000 | Loss: 819.7462768554688 | Test loss: 819.235595703125\n",
      "Epoch: 0 | Loss: 3430.759521484375 | Test loss: 3425.744384765625\n",
      "Epoch: 1000 | Loss: 1327.1729736328125 | Test loss: 991.6559448242188\n",
      "Epoch: 2000 | Loss: 1243.974853515625 | Test loss: 977.0578002929688\n",
      "Epoch: 3000 | Loss: 1162.905029296875 | Test loss: 960.5809326171875\n",
      "Epoch: 4000 | Loss: 1081.837890625 | Test loss: 945.1845703125\n",
      "Epoch: 5000 | Loss: 1015.1168212890625 | Test loss: 932.600341796875\n",
      "Epoch: 6000 | Loss: 967.031005859375 | Test loss: 923.3988647460938\n",
      "Epoch: 7000 | Loss: 937.1676635742188 | Test loss: 917.5636596679688\n",
      "Epoch: 8000 | Loss: 921.4110107421875 | Test loss: 914.5640869140625\n",
      "Epoch: 9000 | Loss: 915.3517456054688 | Test loss: 913.4650268554688\n",
      "Epoch: 10000 | Loss: 914.0184936523438 | Test loss: 913.2658081054688\n",
      "Epoch: 11000 | Loss: 913.9047241210938 | Test loss: 913.2572631835938\n",
      "Epoch: 12000 | Loss: 913.9029541015625 | Test loss: 913.2581787109375\n",
      "Epoch: 13000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 14000 | Loss: 913.9033203125 | Test loss: 913.25830078125\n",
      "Epoch: 15000 | Loss: 913.9033203125 | Test loss: 913.25830078125\n",
      "Epoch: 16000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 17000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 18000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 19000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 20000 | Loss: 913.9033813476562 | Test loss: 913.25830078125\n",
      "Epoch: 0 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 1000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 2000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 3000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 4000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 5000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 6000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 7000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 8000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 9000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 10000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 11000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 12000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 13000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 14000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 15000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 16000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 17000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 18000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 19000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 20000 | Loss: 3456.14453125 | Test loss: 3451.86865234375\n",
      "Epoch: 0 | Loss: 3451.97412109375 | Test loss: 3447.410888671875\n",
      "Epoch: 1000 | Loss: 3195.821044921875 | Test loss: 3191.52392578125\n",
      "Epoch: 2000 | Loss: 2999.431884765625 | Test loss: 2995.3740234375\n",
      "Epoch: 3000 | Loss: 2852.022216796875 | Test loss: 2848.182373046875\n",
      "Epoch: 4000 | Loss: 2746.072509765625 | Test loss: 2742.43212890625\n",
      "Epoch: 5000 | Loss: 2675.55712890625 | Test loss: 2672.096923828125\n",
      "Epoch: 6000 | Loss: 2634.42138671875 | Test loss: 2631.1162109375\n",
      "Epoch: 7000 | Loss: 2615.376953125 | Test loss: 2612.19287109375\n",
      "Epoch: 8000 | Loss: 2609.583251953125 | Test loss: 2606.4765625\n",
      "Epoch: 9000 | Loss: 2608.761962890625 | Test loss: 2605.689208984375\n",
      "Epoch: 10000 | Loss: 2608.731201171875 | Test loss: 2605.664794921875\n",
      "Epoch: 11000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 12000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 13000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 14000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 15000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 16000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 17000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 18000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 19000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 20000 | Loss: 2608.73095703125 | Test loss: 2605.665283203125\n",
      "Epoch: 0 | Loss: 3413.932373046875 | Test loss: 3408.706298828125\n",
      "Epoch: 1000 | Loss: 2137.4150390625 | Test loss: 1869.3309326171875\n",
      "Epoch: 2000 | Loss: 2044.78857421875 | Test loss: 1837.942138671875\n",
      "Epoch: 3000 | Loss: 1951.4962158203125 | Test loss: 1811.10986328125\n",
      "Epoch: 4000 | Loss: 1876.18115234375 | Test loss: 1789.2576904296875\n",
      "Epoch: 5000 | Loss: 1822.1514892578125 | Test loss: 1773.3563232421875\n",
      "Epoch: 6000 | Loss: 1787.4207763671875 | Test loss: 1762.978515625\n",
      "Epoch: 7000 | Loss: 1768.3072509765625 | Test loss: 1757.135009765625\n",
      "Epoch: 8000 | Loss: 1760.1304931640625 | Test loss: 1754.77294921875\n",
      "Epoch: 9000 | Loss: 1758.3248291015625 | Test loss: 1754.58984375\n",
      "Epoch: 10000 | Loss: 1758.3656005859375 | Test loss: 1754.60693359375\n",
      "Epoch: 11000 | Loss: 1758.307861328125 | Test loss: 1754.6060791015625\n",
      "Epoch: 12000 | Loss: 1758.315185546875 | Test loss: 1754.5574951171875\n",
      "Epoch: 13000 | Loss: 1758.29833984375 | Test loss: 1754.570556640625\n",
      "Epoch: 14000 | Loss: 1758.30078125 | Test loss: 1754.65283203125\n",
      "Epoch: 15000 | Loss: 1758.315673828125 | Test loss: 1754.65771484375\n",
      "Epoch: 16000 | Loss: 1758.2310791015625 | Test loss: 1754.58056640625\n",
      "Epoch: 17000 | Loss: 1758.30419921875 | Test loss: 1754.57177734375\n",
      "Epoch: 18000 | Loss: 1758.23193359375 | Test loss: 1754.5714111328125\n",
      "Epoch: 19000 | Loss: 1758.287841796875 | Test loss: 1754.5284423828125\n",
      "Epoch: 20000 | Loss: 1758.361328125 | Test loss: 1754.71826171875\n",
      "Epoch: 0 | Loss: 3414.970458984375 | Test loss: 3412.932373046875\n",
      "Epoch: 1000 | Loss: 1142.0421142578125 | Test loss: 691.1009521484375\n",
      "Epoch: 2000 | Loss: 967.0269775390625 | Test loss: 623.5358276367188\n",
      "Epoch: 3000 | Loss: 811.6557006835938 | Test loss: 577.152099609375\n",
      "Epoch: 4000 | Loss: 684.3243408203125 | Test loss: 540.0510864257812\n",
      "Epoch: 5000 | Loss: 592.1804809570312 | Test loss: 512.8948364257812\n",
      "Epoch: 6000 | Loss: 533.3516845703125 | Test loss: 495.1796569824219\n",
      "Epoch: 7000 | Loss: 500.6934509277344 | Test loss: 485.6873779296875\n",
      "Epoch: 8000 | Loss: 488.0545654296875 | Test loss: 481.8921203613281\n",
      "Epoch: 9000 | Loss: 485.20440673828125 | Test loss: 481.61151123046875\n",
      "Epoch: 10000 | Loss: 484.9117431640625 | Test loss: 481.632080078125\n",
      "Epoch: 11000 | Loss: 484.9132080078125 | Test loss: 481.7007751464844\n",
      "Epoch: 12000 | Loss: 484.7265930175781 | Test loss: 481.5113525390625\n",
      "Epoch: 13000 | Loss: 485.074462890625 | Test loss: 481.76025390625\n",
      "Epoch: 14000 | Loss: 484.81219482421875 | Test loss: 481.6198425292969\n",
      "Epoch: 15000 | Loss: 484.8319091796875 | Test loss: 481.59954833984375\n",
      "Epoch: 16000 | Loss: 484.812744140625 | Test loss: 481.7278137207031\n",
      "Epoch: 17000 | Loss: 484.9565124511719 | Test loss: 481.6812744140625\n",
      "Epoch: 18000 | Loss: 484.96533203125 | Test loss: 481.52197265625\n",
      "Epoch: 19000 | Loss: 484.8046875 | Test loss: 481.5902404785156\n",
      "Epoch: 20000 | Loss: 484.8954772949219 | Test loss: 481.604736328125\n",
      "Epoch: 0 | Loss: 3435.17626953125 | Test loss: 3430.8974609375\n",
      "Epoch: 1000 | Loss: 1085.1065673828125 | Test loss: 613.6771240234375\n",
      "Epoch: 2000 | Loss: 900.3234252929688 | Test loss: 543.7879638671875\n",
      "Epoch: 3000 | Loss: 735.61474609375 | Test loss: 495.69842529296875\n",
      "Epoch: 4000 | Loss: 609.2743530273438 | Test loss: 457.2206115722656\n",
      "Epoch: 5000 | Loss: 512.5473022460938 | Test loss: 429.2169189453125\n",
      "Epoch: 6000 | Loss: 450.2729797363281 | Test loss: 410.8399963378906\n",
      "Epoch: 7000 | Loss: 416.5875549316406 | Test loss: 400.9139404296875\n",
      "Epoch: 8000 | Loss: 403.2348327636719 | Test loss: 397.0845947265625\n",
      "Epoch: 9000 | Loss: 400.352294921875 | Test loss: 396.6969299316406\n",
      "Epoch: 10000 | Loss: 399.8323669433594 | Test loss: 396.6235656738281\n",
      "Epoch: 11000 | Loss: 400.22418212890625 | Test loss: 396.93182373046875\n",
      "Epoch: 12000 | Loss: 400.06988525390625 | Test loss: 396.7705078125\n",
      "Epoch: 13000 | Loss: 399.9195251464844 | Test loss: 396.7165222167969\n",
      "Epoch: 14000 | Loss: 400.1597900390625 | Test loss: 396.8265075683594\n",
      "Epoch: 15000 | Loss: 400.0083923339844 | Test loss: 396.7385559082031\n",
      "Epoch: 16000 | Loss: 399.9356994628906 | Test loss: 396.8536682128906\n",
      "Epoch: 17000 | Loss: 399.99578857421875 | Test loss: 396.7479553222656\n",
      "Epoch: 18000 | Loss: 400.09320068359375 | Test loss: 396.7815856933594\n",
      "Epoch: 19000 | Loss: 400.02008056640625 | Test loss: 396.8462219238281\n",
      "Epoch: 20000 | Loss: 399.9038391113281 | Test loss: 396.77508544921875\n",
      "Epoch: 0 | Loss: 3411.84619140625 | Test loss: 3406.93115234375\n",
      "Epoch: 1000 | Loss: 2820.114013671875 | Test loss: 2815.81982421875\n",
      "Epoch: 2000 | Loss: 2367.666259765625 | Test loss: 2363.929443359375\n",
      "Epoch: 3000 | Loss: 2029.217041015625 | Test loss: 2025.98779296875\n",
      "Epoch: 4000 | Loss: 1787.095458984375 | Test loss: 1784.3299560546875\n",
      "Epoch: 5000 | Loss: 1627.0694580078125 | Test loss: 1624.7210693359375\n",
      "Epoch: 6000 | Loss: 1534.761962890625 | Test loss: 1532.771484375\n",
      "Epoch: 7000 | Loss: 1492.852294921875 | Test loss: 1491.1396484375\n",
      "Epoch: 8000 | Loss: 1480.543212890625 | Test loss: 1479.0042724609375\n",
      "Epoch: 9000 | Loss: 1478.902099609375 | Test loss: 1477.4349365234375\n",
      "Epoch: 10000 | Loss: 1478.8460693359375 | Test loss: 1477.3934326171875\n",
      "Epoch: 11000 | Loss: 1478.845703125 | Test loss: 1477.3939208984375\n",
      "Epoch: 12000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 13000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 14000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 15000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 16000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 17000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 18000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 19000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 20000 | Loss: 1478.845947265625 | Test loss: 1477.3939208984375\n",
      "Epoch: 0 | Loss: 3425.2578125 | Test loss: 3420.5107421875\n",
      "Epoch: 1000 | Loss: 2990.226806640625 | Test loss: 2985.935302734375\n",
      "Epoch: 2000 | Loss: 2657.534423828125 | Test loss: 2653.6533203125\n",
      "Epoch: 3000 | Loss: 2408.61083984375 | Test loss: 2405.1025390625\n",
      "Epoch: 4000 | Loss: 2230.47998046875 | Test loss: 2227.3125\n",
      "Epoch: 5000 | Loss: 2112.692626953125 | Test loss: 2109.831787109375\n",
      "Epoch: 6000 | Loss: 2044.6998291015625 | Test loss: 2042.1021728515625\n",
      "Epoch: 7000 | Loss: 2013.7911376953125 | Test loss: 2011.39794921875\n",
      "Epoch: 8000 | Loss: 2004.69384765625 | Test loss: 2002.4285888671875\n",
      "Epoch: 9000 | Loss: 2003.4774169921875 | Test loss: 2001.2646484375\n",
      "Epoch: 10000 | Loss: 2003.4354248046875 | Test loss: 2001.2337646484375\n",
      "Epoch: 11000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 12000 | Loss: 2003.4354248046875 | Test loss: 2001.2342529296875\n",
      "Epoch: 13000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 14000 | Loss: 2003.4354248046875 | Test loss: 2001.2342529296875\n",
      "Epoch: 15000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 16000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 17000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 18000 | Loss: 2003.4354248046875 | Test loss: 2001.2342529296875\n",
      "Epoch: 19000 | Loss: 2003.4354248046875 | Test loss: 2001.2342529296875\n",
      "Epoch: 20000 | Loss: 2003.435302734375 | Test loss: 2001.2342529296875\n",
      "Epoch: 0 | Loss: 3426.82763671875 | Test loss: 3421.838134765625\n",
      "Epoch: 1000 | Loss: 1153.9603271484375 | Test loss: 688.1463012695312\n",
      "Epoch: 2000 | Loss: 982.3030395507812 | Test loss: 626.9708251953125\n",
      "Epoch: 3000 | Loss: 823.6071166992188 | Test loss: 580.0601806640625\n",
      "Epoch: 4000 | Loss: 692.3766479492188 | Test loss: 541.9620361328125\n",
      "Epoch: 5000 | Loss: 598.3037109375 | Test loss: 514.0513305664062\n",
      "Epoch: 6000 | Loss: 535.5795288085938 | Test loss: 495.8311462402344\n",
      "Epoch: 7000 | Loss: 502.1360168457031 | Test loss: 485.92095947265625\n",
      "Epoch: 8000 | Loss: 488.1075744628906 | Test loss: 481.9971923828125\n",
      "Epoch: 9000 | Loss: 485.01226806640625 | Test loss: 481.6104736328125\n",
      "Epoch: 10000 | Loss: 484.84661865234375 | Test loss: 481.5960693359375\n",
      "Epoch: 11000 | Loss: 484.8826599121094 | Test loss: 481.6500549316406\n",
      "Epoch: 12000 | Loss: 484.6616516113281 | Test loss: 481.6404724121094\n",
      "Epoch: 13000 | Loss: 484.68389892578125 | Test loss: 481.7825012207031\n",
      "Epoch: 14000 | Loss: 485.0345153808594 | Test loss: 481.6953430175781\n",
      "Epoch: 15000 | Loss: 484.76251220703125 | Test loss: 481.6876220703125\n",
      "Epoch: 16000 | Loss: 485.02923583984375 | Test loss: 481.8819580078125\n",
      "Epoch: 17000 | Loss: 484.88671875 | Test loss: 481.72406005859375\n",
      "Epoch: 18000 | Loss: 484.87261962890625 | Test loss: 481.646728515625\n",
      "Epoch: 19000 | Loss: 485.0010681152344 | Test loss: 481.6526184082031\n",
      "Epoch: 20000 | Loss: 484.9078063964844 | Test loss: 481.5736389160156\n",
      "Epoch: 0 | Loss: 3425.602294921875 | Test loss: 3420.845703125\n",
      "Epoch: 1000 | Loss: 1298.4534912109375 | Test loss: 825.2183227539062\n",
      "Epoch: 2000 | Loss: 1127.0882568359375 | Test loss: 775.7752075195312\n",
      "Epoch: 3000 | Loss: 979.3345336914062 | Test loss: 731.4537353515625\n",
      "Epoch: 4000 | Loss: 851.1498413085938 | Test loss: 694.4598999023438\n",
      "Epoch: 5000 | Loss: 753.913330078125 | Test loss: 666.7074584960938\n",
      "Epoch: 6000 | Loss: 689.2520751953125 | Test loss: 648.0386962890625\n",
      "Epoch: 7000 | Loss: 653.168212890625 | Test loss: 637.3245239257812\n",
      "Epoch: 8000 | Loss: 636.8540649414062 | Test loss: 632.617919921875\n",
      "Epoch: 9000 | Loss: 632.1030883789062 | Test loss: 631.331787109375\n",
      "Epoch: 10000 | Loss: 631.4505615234375 | Test loss: 631.1912231445312\n",
      "Epoch: 11000 | Loss: 631.4329833984375 | Test loss: 631.18994140625\n",
      "Epoch: 12000 | Loss: 631.4321899414062 | Test loss: 631.190673828125\n",
      "Epoch: 13000 | Loss: 631.4322509765625 | Test loss: 631.1904907226562\n",
      "Epoch: 14000 | Loss: 631.4321899414062 | Test loss: 631.1903686523438\n",
      "Epoch: 15000 | Loss: 631.4323120117188 | Test loss: 631.1902465820312\n",
      "Epoch: 16000 | Loss: 631.4319458007812 | Test loss: 631.1902465820312\n",
      "Epoch: 17000 | Loss: 631.4327392578125 | Test loss: 631.18994140625\n",
      "Epoch: 18000 | Loss: 631.43212890625 | Test loss: 631.1902465820312\n",
      "Epoch: 19000 | Loss: 631.4326782226562 | Test loss: 631.1913452148438\n",
      "Epoch: 20000 | Loss: 631.43212890625 | Test loss: 631.1904296875\n",
      "Epoch: 0 | Loss: 3426.184326171875 | Test loss: 3421.237060546875\n",
      "Epoch: 1000 | Loss: 2814.85205078125 | Test loss: 2810.543212890625\n",
      "Epoch: 2000 | Loss: 2346.858154296875 | Test loss: 2343.123291015625\n",
      "Epoch: 3000 | Loss: 1996.2503662109375 | Test loss: 1993.0379638671875\n",
      "Epoch: 4000 | Loss: 1744.90966796875 | Test loss: 1742.17529296875\n",
      "Epoch: 5000 | Loss: 1578.2747802734375 | Test loss: 1575.9705810546875\n",
      "Epoch: 6000 | Loss: 1481.672119140625 | Test loss: 1479.7381591796875\n",
      "Epoch: 7000 | Loss: 1437.427978515625 | Test loss: 1435.7818603515625\n",
      "Epoch: 8000 | Loss: 1424.226318359375 | Test loss: 1422.76220703125\n",
      "Epoch: 9000 | Loss: 1422.4166259765625 | Test loss: 1421.0286865234375\n",
      "Epoch: 10000 | Loss: 1422.351806640625 | Test loss: 1420.979736328125\n",
      "Epoch: 11000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 12000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 13000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 14000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 15000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 16000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 17000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 18000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 19000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n",
      "Epoch: 20000 | Loss: 1422.3515625 | Test loss: 1420.98046875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = MultivariateLinearRegression(input_dim, output_dim, params['hidden_dim'], params['dropout_rate'])\n",
    "    num_epochs = 20001\n",
    "    epoch_count = []\n",
    "    loss_values = []\n",
    "    test_loss_values = []\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        # inputs, labels = enumerate(zip(X_train, y_train))\n",
    "        model.train()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        # print(f\"Loss: {loss}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #testing\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "        # with torch.no_grad()\n",
    "            test_pred = model(X_test)      \n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "        if epoch % 1000 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            loss_values.append(loss)\n",
    "            test_loss_values.append(test_loss)\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# Set up loss fucntion\n",
    "loss_fn = nn.L1Loss() # minimize the distance between predictions and true value... keeps increase weights until weights reduce loss.. decrease the bias and finds loss increase.. increase bias\n",
    "#set up optimizer(stocastic gradient descent)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20001\n",
    "epoch_count = []\n",
    "loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # inputs, labels = enumerate(zip(X_train, y_train))\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # print(f\"Loss: {loss}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "    # with torch.no_grad()\n",
    "        test_pred = model(X_test)      \n",
    "        test_loss = criterion(test_pred, y_test)\n",
    "    if epoch % 1000 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        loss_values.append(loss)\n",
    "        test_loss_values.append(test_loss)\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
    "        # print(model.state_dict())\n",
    "# print(model.linear.weight)\n",
    "# print(model.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label='Train loss')\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and Test Loss Curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a6141bb765aa3bbac06bf2c44817d253d74b01a0fc0a193c6da1ab26e07cb0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
