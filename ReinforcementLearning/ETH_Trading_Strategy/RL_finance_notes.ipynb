{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning - Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Components:\n",
    "* Agent = Trading\n",
    "* Action = Buy / Hold / Sell\n",
    "* Reward = realized profits, Loss, Sharpe Ratio, etc\n",
    "* Environment = Stock Market Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important framework:\n",
    "* Bellman Equation:\\\n",
    "-Two parts: immediate reward and discounted value of previous state\\\n",
    "-you find state-action values (Q-values) and value function\\\n",
    "-important bc they let us express values of states as values of other states\n",
    "* Markov Decision Process (MDP)\\\n",
    "-S = set of states\\\n",
    "-A = set of Action\\\n",
    "-P = transition probability\\\n",
    "-R = reward\\\n",
    "-gamma = discount factor for future rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically MDP frame the agent-environment interaction as a sequential decision problem over a series of time steps from t=1,...,t=T\\\n",
    "The agent and the environment interact continually, agent selects action and the environment responds towards the action and provides new situation to the agent with the goal of giving strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-Free Algorithm...\n",
    "* Value-based (i.e: Q-learning, SARSA, DQN)\n",
    "* Policy-based (i.e: Policy Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning = adaptation of TD learning... algorithm evaluates which action to take based on Q-value (action value) function that determinesthe value of being in a certain state and taking certain action at that current state. Then for each state-action pair, it will keep track of the running avg of rewards, which the agent gets after leaving the state with the action, plus the rewards it gets to earn later. \\\n",
    "Q-learning = an off-policy type of algorithm\n",
    "1) At time step t=1, we start from state s, then pick an action based on Q-values\n",
    "2) We can apply epsilon-greedy approach that selects an action randomly with epsilon OR choose based on Q-value function\n",
    "3) After the action, we observe the reward and go to the next state t+1\n",
    "4) update the action-value function, then follow the t+1 sequentially..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages: might have instability and some Q-values might lead towards divergence... and if state/action space are large, the Q-value table might be infeasible.\n",
    "Solution: use Artificial Neural Network to approximate Q-values... \\\n",
    "THIS IS THE DEEP Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-learning Algorithm approximates Q-values by learning sets of weights through series of hidden layers in the Deep Q-network that maps states to action... the algorithm aims to stabilize the training process of Q-learning through:\n",
    "* Experience replay\n",
    "* Periodic target update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA = TD learning-based algorithm... update Q-values by folowing sequence of State -> Action -> Reward (t+1) -> State (t+1), Action (t+1), ...\\\n",
    "SARSA = on-policy algorithm... agent grasp the optimal policy and recycles it. Policy for updating and the action are the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mytfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a6141bb765aa3bbac06bf2c44817d253d74b01a0fc0a193c6da1ab26e07cb0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
